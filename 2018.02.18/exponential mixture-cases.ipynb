{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from projectors import *\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "import read_matpower\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import pypower.api as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Pegase isn't valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winter Peak 2383"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(os.path.abspath(os.curdir), 'matpower6.0/case2383wp.m')\n",
    "baseMVA, bus_data, generator_data, branch_data, cost_data = read_matpower.preprocess(filename)\n",
    "bus_df, gen_df, branch_df = read_matpower.DF_converter(bus_data, generator_data, branch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_b = np.pi / 2.0\n",
    "bus_dict = read_matpower.bus_dict_constructor(bus_df)\n",
    "G, K, mu, sigma = read_matpower.constraints_constructor(baseMVA, bus_df, gen_df, branch_df, theta_bound=theta_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W, tau = read_matpower.adjust_constraints((G,K), (mu, sigma))\n",
    "#tau = -tau\n",
    "#W = -W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for calculating probabilities to get into each half-spaces\n",
    "\n",
    "def calculate_probas(W,tau):\n",
    "    return norm.cdf(-tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probas = calculate_probas(W, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# elimination of values that are too rare (so that further computations become possible)\n",
    "critical_value = 1e-300\n",
    "valid_idx = np.argwhere(probas >= critical_value)[:,0]\n",
    "probas = probas[valid_idx]\n",
    "W = W[valid_idx]\n",
    "tau = tau[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIM_X = W.shape[1]\n",
    "NUM_B = W.shape[0]\n",
    "eps = 0.1 / (NUM_B + 1) # lower bound which prevents alpha values from being equal to zero\n",
    "\n",
    "\n",
    "\n",
    "#W = np.random.randn(NUM_B, DIM_X)\n",
    "#W = np.apply_along_axis(lambda x: x / np.linalg.norm(x), 1, W)\n",
    "#tau = np.fabs(np.random.randn(NUM_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function which return 1 if x lies in some definite halfspace and 0 otherwise\n",
    "\n",
    "def boundaries(x, W, tau):\n",
    "    return (W.dot(x) >= tau).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boundaries_wrap = lambda x: boundaries(x, W, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating samples conditioning on one of halspaces\n",
    "\n",
    "def generate_sample(W, tau, mix_index):\n",
    "    if mix_index == len(W):\n",
    "        return np.random.randn(DIM_X)\n",
    "    w = W[mix_index]\n",
    "    t = tau[mix_index]\n",
    "    z = np.random.randn(DIM_X)\n",
    "    u = np.random.rand()\n",
    "    y = u * norm.cdf(-t)\n",
    "    y = norm.ppf(y)\n",
    "    return -y*w - z + (w.dot(z)) * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_wrap = lambda i: generate_sample(W, tau, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Mixture():\n",
    "    '''Class operating with distribution mixtures, \n",
    "    derived from some base distribution function conditioning on different halfspaces'''\n",
    "    def __init__(self, dist_func, boundaries, probas, gen_funcs, block = True):\n",
    "        self.base_dist = dist_func # base distribution function\n",
    "        self.boundaries = boundaries \n",
    "        self.probas = probas\n",
    "        self.gen_funcs = gen_funcs \n",
    "        self.valid_idx = np.argwhere(self.probas != 0)[:,0] #in this implementation aren't required\n",
    "        self.block = block #in this implementation aren't required\n",
    "\n",
    "    '''def block_invalid_boundaries(boundaries):\n",
    "        return boundaries[self.valid_idx]'''\n",
    "            \n",
    "    def set_alpha(self, alpha):\n",
    "        '''Function to change alpha which class is operating '''\n",
    "        self.alpha = alpha\n",
    "        #if self.block:\n",
    "        #    self.alpha = self.alpha[self.valid_idx]\n",
    "        #    norm = self.alpha.sum()\n",
    "        #    self.alpha = self.alpha / norm\n",
    "        \n",
    "    def mix_pdf(self, x):\n",
    "        '''probability density functions of all distributions which are present in the mixture''' \n",
    "        #if self.block:\n",
    "        #    return self.base_dist(x) * self.boundaries(x)[self.valid_idx] / self.probas[self.valid_idx]\n",
    "        return self.base_dist(x) * self.boundaries(x) / self.probas\n",
    "    \n",
    "    '''def mix_dist(self, x):\n",
    "        res = 0\n",
    "        for i, a in enumerate(self.alpha):\n",
    "            if i != len(self.alpha) - 1:\n",
    "                res += a*self.base_dist(x)*self.boundaries(i, x)/self.probas[i]\n",
    "            else:\n",
    "                res += a*self.base_dist(x)\n",
    "        return res'''\n",
    "\n",
    "    def mix_dist(self, x, block = True):\n",
    "        '''probability density function of the whole mixture'''\n",
    "        probas = self.probas\n",
    "        bounds = self.boundaries(x)\n",
    "        #if self.block:\n",
    "        #    probas = self.probas[self.valid_idx]\n",
    "        #    bounds = self.boundaries(x)[self.valid_idx]\n",
    "        p = self.base_dist(x)\n",
    "        coef = 1 / probas\n",
    "        coef = p * coef\n",
    "        bounds = bounds* coef\n",
    "        print(bounds)\n",
    "        res = self.alpha[:-1].dot(bounds)\n",
    "        res += self.alpha[-1] * p\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def sample(self):\n",
    "        n = len(self.alpha)\n",
    "        choice = np.random.choice(n, p=self.alpha)\n",
    "        #if self.block:\n",
    "        #    choice = self.valid_idx[choice]\n",
    "        return self.gen_funcs(choice)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_func = lambda x: multivariate_normal.pdf(x, np.zeros((DIM_X)), np.identity(DIM_X))\n",
    "mixture = Mixture(dist_func, boundaries_wrap, probas, generate_wrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First optimization task\n",
    "\n",
    "$$\n",
    "\\text{Var } \\hat \\mu_{\\alpha} = \\frac 1n \\left(\\int \\frac{H_{1:J}(x) p(x)}{\\sum_{j=0}^J \\alpha_j H_j (x) P_j^{-1}} \\, dx  - \\mu^2\\right)\\longrightarrow \\min_{\\alpha}\n",
    "$$\n",
    "\n",
    "We are going to use stochastic gradient descent. The stochastic gradient of this task, having $X_i \\sim q_{\\alpha}(x)$ is $-\\dfrac{H_{1:J}(X_i) p^2(X_i) \\vec q (X_i)}{q_{\\alpha}^3(X_i)}$\n",
    "\n",
    "Estimation of the variance above:\n",
    "$$\n",
    "\\frac 1n \\sum_{i = 1}^n \\frac{H_{1:J}(x_i) p^2(x_i)}{q_{\\alpha} (x_i) q_{\\alpha'}(x_i)} \\longrightarrow \\min_{\\alpha}\n",
    "$$\n",
    "where $x_i \\sim q_{\\alpha'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Owen's implementation described in https://arxiv.org/pdf/1411.3954.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simul_num1 = 10**4\n",
    "simul_num2 = 10**5\n",
    "dim_alpha = NUM_B + 1\n",
    "dim_beta = NUM_B\n",
    "dim_gamma = NUM_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main function description which we want to minimize\n",
    "def calculate_coefs(mixture, simul_num = 10**4):\n",
    "    bound_num = len(mixture.alpha)\n",
    "    coefs_x = np.zeros((simul_num, bound_num))\n",
    "    coefs_y = np.zeros((simul_num))\n",
    "    coefs_z = np.zeros((simul_num))\n",
    "    alpha = mixture.alpha\n",
    "    for i in range(simul_num):\n",
    "        x = mixture.sample()\n",
    "        h = 0\n",
    "        if np.any(mixture.boundaries(x)):\n",
    "            h = 1\n",
    "        coefs_y[i] = h\n",
    "        coefs_x[i, :-1] = mixture.boundaries(x) / mixture.probas\n",
    "        coefs_x[i, -1] = 1\n",
    "        coefs_z[i] =  alpha.dot(coefs_x[i,:])\n",
    "    coefs = (coefs_x, coefs_y, coefs_z)\n",
    "    return coefs\n",
    "\n",
    "def func(coefs, alpha, beta = None, gamma = None, simul_num = 10**4, sample_num = 10**5):\n",
    "    bound_num = len(alpha)\n",
    "    if np.any(beta) == None:\n",
    "        beta = np.zeros((bound_num - 1))\n",
    "    if np.any(gamma) == None:\n",
    "        gamma = np.zeros((bound_num - 1))\n",
    "    x, y, z = coefs\n",
    "    alpha = alpha.reshape((-1, 1))\n",
    "    to_sum = x.dot(alpha)\n",
    "    to_sum = to_sum.ravel()\n",
    "    to_sum = np.divide(np.ones((to_sum.size,)), to_sum)\n",
    "    normalizer = y - x[:,:-1].dot(beta) + gamma.sum()\n",
    "    to_sum = to_sum * normalizer\n",
    "    to_sum = to_sum * (normalizer / z)\n",
    "    temp = (gamma - beta).sum()\n",
    "    return to_sum.sum() - 2*(y / z).sum()*temp + (sample_num -1)*(temp**2)*(simul_num) ### to check\n",
    "\n",
    "def grad_func(coefs, alpha, beta = None, gamma = None, simul_num = 10**4, sample_num = 10**5):\n",
    "    bound_num = len(alpha)\n",
    "    checker = np.any(beta) == None\n",
    "    if checker:\n",
    "        beta = np.zeros((bound_num - 1))\n",
    "        gamma = np.zeros((bound_num - 1))\n",
    "    x,y,z = coefs\n",
    "    alpha = alpha.reshape((-1, 1))\n",
    "    norm = y - x[:,:-1].dot(beta) + gamma.sum()\n",
    "    normalizer = norm / z\n",
    "    norm = normalizer * norm\n",
    "    res = x.dot(alpha)\n",
    "    #res = res **2\n",
    "    #print(len(res[res == np.inf]))\n",
    "    coef = - x / res\n",
    "    x = coef / res\n",
    "\n",
    "    grad_a = (x.T.dot(norm)).ravel()\n",
    "    if checker:\n",
    "        return grad_a\n",
    "    temp = (gamma - beta).sum()\n",
    "    mu = (y / z).sum()\n",
    "    grad_b = ((coef[:,:-1]).T.dot(normalizer)).ravel() + mu - (sample_num - 1) * temp*simul_num\n",
    "    grad_b = 2 * grad_b\n",
    "    grad_g = normalizer.dot(1 / res.ravel()) - mu + (sample_num - 1) * temp*simul_num\n",
    "    grad_g = 2* grad_g\n",
    "    grad_g = np.ones((bound_num - 1)) * grad_g\n",
    "    return  np.hstack((grad_a, grad_b, grad_g))\n",
    "\n",
    "def hess_func(coefs, alpha, beta = None, gamma = None, simul_num = 10**4, sample_num = 10**5):\n",
    "    bound_num = len(alpha)    \n",
    "    checker = np.any(beta) == None\n",
    "    if checker:\n",
    "        beta = np.zeros((bound_num - 1))\n",
    "        gamma = np.zeros((bound_num - 1))\n",
    "    x,y,z = coefs\n",
    "    y = y.reshape((-1,1))\n",
    "    z = z.reshape((-1,1))\n",
    "    norm = y - x[:,:-1].dot(beta.reshape((-1,1))) + gamma.sum()\n",
    "    normalizer = norm / z\n",
    "    norm = normalizer * norm\n",
    "    alpha = alpha.reshape((-1, 1))\n",
    "    res = x.dot(alpha)\n",
    "    coefs = x / res\n",
    "    hess_a = 2 * (coefs * norm).T.dot(coefs / res)\n",
    "    if checker:\n",
    "        return hess_a\n",
    "    hess_ab = 2 * (coefs * normalizer).T.dot(coefs[:, :-1] / res)\n",
    "    #hess_ag = np.zeros((len(gamma), 1))\n",
    "    hess_ag = -2 * (coefs * normalizer).T.dot(1 / res)\n",
    "    hess_ag = np.repeat(hess_ag, len(gamma), axis = 1)\n",
    "    hess_b = 2 * (coefs[:, :-1]).T.dot(x[:, :-1] / z) + 2 * (sample_num - 1)* simul_num\n",
    "    hess_bg = -2 * (coefs[:,:-1]).T.dot(1 / z) - 2 * (sample_num - 1) * simul_num\n",
    "    hess_bg = np.repeat(hess_bg, len(gamma), axis = 1)\n",
    "    hess_g = 2*((1 / res) / z).sum() + 2 * (sample_num - 1)* simul_num\n",
    "    hess_g = hess_g * np.ones((len(gamma), len(gamma)))\n",
    "    hess_a = np.hstack((hess_a, hess_ab, hess_ag))\n",
    "    hess_b = np.hstack((hess_ab.T, hess_b, hess_bg))\n",
    "    hess_g = np.hstack((hess_ag.T, hess_bg.T, hess_g))\n",
    "    return np.vstack((hess_a, hess_b, hess_g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = 1e-2\n",
    "delta = (1 + eta - (NUM_B + 1)*eps) / (NUM_B + 2)\n",
    "zero_alpha = (eps + delta) * np.ones((NUM_B + 1)) # zero alpha for optimization\n",
    "sample_alpha = np.ones((NUM_B + 1)) / (NUM_B + 1) #alpha which will be used for the first simulation stage\n",
    "sample_alpha[np.argmax(sample_alpha)] += 1 - sample_alpha.sum()\n",
    "mixture.set_alpha(sample_alpha)\n",
    "coefs = calculate_coefs(mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# addition of log barriers\n",
    "def log_barrier(bound_values, rho = 1):\n",
    "    bound_values = np.log(bound_values)\n",
    "    return - rho * np.sum(bound_values)\n",
    "\n",
    "def big_func(coefs, alpha, beta = None, gamma = None, simul_num = 10**4, sample_num = 10**5, rho = 1, eta = 0.01):\n",
    "    bound_values = alpha - eps\n",
    "    bound_values = np.hstack((bound_values, np.array(1 + eta - np.sum(alpha))))\n",
    "    res = func(coefs, alpha, beta, gamma, simul_num, sample_num) + log_barrier(bound_values, rho)\n",
    "    return res\n",
    "\n",
    "def big_grad(coefs, alpha, beta = None, gamma = None, simul_num = 10**4, sample_num = 10**5, rho = 1, eta = 0.01):\n",
    "    bound_values = alpha - eps\n",
    "    bound_values = 1.0 / bound_values\n",
    "    upper_bound = 1 + eta - np.sum(alpha)\n",
    "    upper_bound = 1.0 / upper_bound\n",
    "    barrier_part = (upper_bound - bound_values) * rho\n",
    "    gr = grad_func(coefs, alpha, beta, gamma, simul_num, sample_num)\n",
    "    if np.any(beta) == None:\n",
    "        return barrier_part + gr\n",
    "    else:\n",
    "        return np.hstack((barrier_part, np.zeros((gr.size - barrier_part.size)))) + gr\n",
    "\n",
    "def big_hess(coefs, alpha, beta = None, gamma = None, simul_num = 10**4, sample_num = 10**5, rho = 1, eta = 0.01):\n",
    "    bound_values = alpha - eps\n",
    "    bound_num = len(alpha)\n",
    "    bound_values = 1.0 / (bound_values)**2\n",
    "    upper_bound = 1 + eta - np.sum(alpha)\n",
    "    upper_bound = 1.0 / upper_bound ** 2\n",
    "    barrier_part = (np.diag(bound_values) + np.ones((alpha.size, alpha.size)) * upper_bound) * rho\n",
    "    he = hess_func(coefs, alpha, beta, gamma, simul_num, sample_num)\n",
    "    b_p = np.zeros(he.shape)\n",
    "    b_p[:bound_num, :bound_num] = barrier_part\n",
    "    return b_p + he"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search for the appropriate step size for the given direction\n",
    "def backtracking_line_search(direction, func, grad_func, dom_func, point, update, alpha = 0.3, beta = 0.8):\n",
    "    t = 1\n",
    "    x = point\n",
    "    f_x = func(x)\n",
    "    gr_x = grad_func(x)\n",
    "    while not dom_func(update(x,direction, t)):\n",
    "        #print(\"2. jkjdf\")\n",
    "        t = beta*t\n",
    "    while func(update(x, direction, t)) > f_x + alpha*t*(gr_x.dot(direction)):\n",
    "        #print(\"3. jdkfjldj\")\n",
    "        t = beta*t\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update1(x, direction, t):\n",
    "    return x + t*direction.ravel()\n",
    "    \n",
    "### This function tends to stick in a cycle with some eta, to be checked later\n",
    "\n",
    "def damped_newton(func, grad_func, hess, dom_func, point, update, tolerance = 1e-4):\n",
    "    x = point\n",
    "    while True:\n",
    "        grad = grad_func(x).reshape((-1,1))\n",
    "        hess_inv = np.linalg.pinv(hess(x))\n",
    "        direction = - hess_inv.dot(grad)\n",
    "        decrement = grad.T.dot(hess_inv)\n",
    "        decrement = decrement.dot(grad)\n",
    "        #print(decrement)\n",
    "        if decrement**2 / 2 <= tolerance:\n",
    "            break\n",
    "        print(decrement**2/2)\n",
    "        t = backtracking_line_search(direction.ravel(), func, grad_func, dom_func, x, update=update)\n",
    "        x = update(x, direction.ravel(), t) \n",
    "        #x = tuple(x[i] + t*direction.ravel()[i*len(x[0]): ]) x + t* direction.ravel()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho: 1\n",
      "[[ 62.76110973]]\n",
      "[[ 9.89890363]]\n",
      "[[ 0.36309992]]\n",
      "[[ 0.00063107]]\n",
      "rho: 0.5\n",
      "[[ 0.78759601]]\n",
      "[[ 0.00517167]]\n",
      "rho: 0.25\n",
      "[[ 0.55511009]]\n",
      "[[ 0.00477218]]\n",
      "rho: 0.125\n",
      "[[ 0.34807895]]\n",
      "[[ 0.00323376]]\n",
      "rho: 0.0625\n",
      "[[ 0.2044862]]\n",
      "[[ 0.00174154]]\n",
      "rho: 0.03125\n",
      "[[ 0.11484752]]\n",
      "[[ 0.00073145]]\n",
      "rho: 0.015625\n",
      "[[ 0.06244116]]\n",
      "[[ 0.00021427]]\n",
      "rho: 0.0078125\n",
      "[[ 0.03310663]]\n",
      "rho: 0.00390625\n",
      "[[ 0.03047393]]\n",
      "rho: 0.001953125\n",
      "[[ 0.01369975]]\n",
      "rho: 0.0009765625\n",
      "[[ 0.00277256]]\n",
      "rho: 0.00048828125\n",
      "[[ 0.00031701]]\n",
      "rho: 0.000244140625\n",
      "rho: 0.0001220703125\n",
      "[[ 0.00302303]]\n",
      "rho: 6.103515625e-05\n",
      "rho: 3.0517578125e-05\n",
      "rho: 1.52587890625e-05\n",
      "[[ 0.00026549]]\n",
      "rho: 7.62939453125e-06\n",
      "rho: 3.814697265625e-06\n",
      "rho: 1.9073486328125e-06\n",
      "rho: 9.5367431640625e-07\n",
      "rho: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "#eta = 1e-3\n",
    "tol_e = 1e-4\n",
    "rho = 1\n",
    "dom_func = lambda x: np.all(x >= eps) & (x.sum() < 1 + eta)\n",
    "params_to_opt = zero_alpha\n",
    "while True:\n",
    "    print('rho:', rho)\n",
    "    func_wrap = lambda x: big_func(coefs, x, rho = rho)\n",
    "    grad_wrap = lambda x: big_grad(coefs, x, rho = rho)\n",
    "    hess_wrap = lambda x: big_hess(coefs, x, rho = rho)\n",
    "    params_to_opt = damped_newton(func_wrap, grad_wrap, hess_wrap, dom_func, params_to_opt, update=update1)\n",
    "    if (NUM_B + 2) * rho < func(coefs, params_to_opt) * tol_e:\n",
    "        break\n",
    "    else: rho = rho / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_estimation(mixture, projector, alpha, beta = None, gamma = None, sample_num = 10**5):\n",
    "    bound_num = len(mixture.alpha)\n",
    "    if np.any(beta) == None:\n",
    "        beta = np.zeros((bound_num - 1))\n",
    "        gamma = np.zeros((bound_num - 1))\n",
    "    alpha = projector(alpha)\n",
    "    alpha[np.argmax(alpha)] += 1 - alpha.sum()\n",
    "    mixture.set_alpha(alpha)\n",
    "    x,y,z = calculate_coefs(mixture, sample_num)\n",
    "    nom = y - x[:, :-1].dot(beta) + gamma.sum()\n",
    "    denom = x.dot(alpha)\n",
    "    est = np.mean(nom / denom)\n",
    "    return est - (gamma - beta).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01035304508895396"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = SimplexProjector(eps)\n",
    "get_estimation(mixture, proj, params_to_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound of estimation 0.010396508968529443 \n",
      " lower bound of estimation 0.0012844157635156065\n"
     ]
    }
   ],
   "source": [
    "print('upper bound of estimation {0} \\n lower bound of estimation {1}'.format(probas.sum(), probas.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update2(x, direction):\n",
    "    sizes = tuple(len(x_i) for x_i in x)\n",
    "    start_idx = []\n",
    "    idx = 0\n",
    "    for i in range(len(x)):\n",
    "        start_idx.append(idx)\n",
    "        idx += sizes[i]\n",
    "    return [x[i] + direction[start_idx[i]: start_idx[i] + sizes[i]] for i in range(len(x))]\n",
    "\n",
    "#######################################################333\n",
    "#SHOULD BE FINISHED LATER\n",
    "#########################################################\n",
    "\n",
    "def sgd(objective, mixture, grad, zero_point, projector, C = 1, max_iter = 10**5, update = update2, momentum = None):\n",
    "    points = []\n",
    "    checker = momentum == None\n",
    "    cur_point = zero_point[:]\n",
    "    estim = []\n",
    "    points.append(cur_point)\n",
    "    cumsum = 0\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        #sample part\n",
    "        try:\n",
    "            mixture.set_alpha(cur_point[0])\n",
    "            cur_x = mixture.sample()\n",
    "        except:\n",
    "            cur_point[0][np.argmax(cur_point[0])] += 1 - np.sum(cur_point[0])\n",
    "            mixture.set_alpha(cur_point[0])\n",
    "            cur_x = mixture.sample()\n",
    "        x = cur_x\n",
    "        \n",
    "        # estimation part\n",
    "        if len(cur_point) > 1:\n",
    "            bias = (cur_point[2] - cur_point[1]).sum()\n",
    "        else:\n",
    "            bias = 0\n",
    "        \n",
    "        tmp = objective(x, cur_point)\n",
    "        \n",
    "        cumsum += tmp - bias\n",
    "        estim.append(cumsum / (i + 1))\n",
    "        \n",
    "        #gradient part\n",
    "        if (checker) | (i == 0):\n",
    "            gradient = grad(x, cur_point)\n",
    "        else:\n",
    "            direction = momentum * direction \n",
    "            gradient = grad(x, update(cur_point, direction))\n",
    "        \n",
    "        # step part\n",
    "        step_size = C/np.sqrt(i+1)\n",
    "        #step_size = C\n",
    "        if (checker) | (i == 0):\n",
    "            direction = -gradient * step_size\n",
    "        else:\n",
    "            direction -= gradient * step_size\n",
    "        \n",
    "        cur_point = update(cur_point, direction) ######################## check!!!bb\n",
    "        \n",
    "        #alert if some problems with alpha occurs\n",
    "        if np.any(np.isinf(cur_point[0])):\n",
    "            print('Iteration # {}: infinity trouble'.format(i + 1))\n",
    "            print(cur_point[0])\n",
    "            print(cur_point[1])\n",
    "            print(cur_point[2])\n",
    "        if np.any(np.isnan(cur_point[0])):\n",
    "            print('Iteration # {}: nan trouble'.format(i + 1))\n",
    "            \n",
    "        #projector part\n",
    "        cur_point[0] = projector(cur_point[0]) ################# necessary to check later\n",
    "    \n",
    "        points.append(cur_point)            \n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"Iteration #{}\".format(i + 1))\n",
    "    return np.array(estim), points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def obj1(x, mixture, alpha, beta = None, gamma = None):\n",
    "    bound_num = len(alpha)\n",
    "    if np.any(beta) == None:\n",
    "        beta = np.zeros((bound_num - 1))\n",
    "        gamma = np.zeros((bound_num - 1))\n",
    "    h = 0\n",
    "    bounds = mixture.boundaries(x)\n",
    "    if np.any(bounds):\n",
    "        h = 1\n",
    "    bounds = bounds / mixture.probas\n",
    "    nom = h - bounds.dot(beta) + gamma.sum()\n",
    "    denom = bounds.dot(alpha[:-1]) + alpha[-1]\n",
    "    return nom / denom\n",
    "\n",
    "def grad1(x, mixture, alpha, beta = None, gamma = None, sample_num = 10**5):\n",
    "    bound_num = len(alpha)\n",
    "    checker = np.any(beta) == None\n",
    "    if checker:\n",
    "        beta = np.zeros((bound_num - 1))\n",
    "        gamma = np.zeros((bound_num - 1))\n",
    "    h = 0\n",
    "    bounds = mixture.boundaries(x)\n",
    "    if np.any(bounds):\n",
    "        h = 1\n",
    "    bounds = bounds / mixture.probas\n",
    "    nom = h - bounds.dot(beta) + gamma.sum()\n",
    "    denom = bounds.dot(alpha[:-1]) + alpha[-1]\n",
    "    frac = nom / denom\n",
    "    grad_a = np.ones((bound_num))\n",
    "    grad_a = grad_a*((frac)**2)\n",
    "    grad_a[:-1] = grad_a[:-1] * bounds\n",
    "    grad_a = -grad_a / denom\n",
    "    if checker:\n",
    "        return grad_a\n",
    "    \n",
    "    if np.any(np.isinf(grad_a)):\n",
    "        print('infinity trouble with grad_a')\n",
    "    if np.any(np.isnan(grad_a)):\n",
    "        print('nan trouble with grad_a')\n",
    "\n",
    "    mu = h / denom\n",
    "    grad_b = frac * (bounds / denom)\n",
    "    grad_b -= mu\n",
    "    grad_b = grad_b * (-2) +  beta\n",
    "    if np.any(np.isinf(grad_b)):\n",
    "        print('infinity trouble with grad_b')\n",
    "    if np.any(np.isnan(grad_b)):\n",
    "        print('nan trouble with grad_b')\n",
    "        \n",
    "    grad_g = frac * (1 / denom) - mu \n",
    "    grad_g = grad_g * 2\n",
    "    grad_g = np.ones((bound_num - 1)) * grad_g +  gamma\n",
    "    if np.any(np.isinf(grad_g)):\n",
    "        print('infinity trouble with grad_g')\n",
    "    if np.any(np.isnan(grad_g)):\n",
    "        print('nan trouble with grad_g')\n",
    "    return np.hstack((grad_a, grad_b, grad_g))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = lambda x, params: obj1(x, mixture, params[0])\n",
    "grad = lambda x, params: grad1(x, mixture, params[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_p = (1 / (NUM_B + 1)) * np.ones((NUM_B + 1))\n",
    "proj = SimplexProjector(epsilon = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_p = [zero_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1000\n",
      "Iteration #2000\n",
      "Iteration #3000\n",
      "Iteration #4000\n",
      "Iteration #5000\n",
      "Iteration #6000\n",
      "Iteration #7000\n",
      "Iteration #8000\n",
      "Iteration #9000\n",
      "Iteration #10000\n",
      "Iteration #11000\n",
      "Iteration #12000\n",
      "Iteration #13000\n",
      "Iteration #14000\n",
      "Iteration #15000\n",
      "Iteration #16000\n",
      "Iteration #17000\n",
      "Iteration #18000\n",
      "Iteration #19000\n",
      "Iteration #20000\n",
      "Iteration #21000\n",
      "Iteration #22000\n",
      "Iteration #23000\n",
      "Iteration #24000\n",
      "Iteration #25000\n",
      "Iteration #26000\n",
      "Iteration #27000\n",
      "Iteration #28000\n",
      "Iteration #29000\n",
      "Iteration #30000\n",
      "Iteration #31000\n",
      "Iteration #32000\n",
      "Iteration #33000\n",
      "Iteration #34000\n",
      "Iteration #35000\n",
      "Iteration #36000\n",
      "Iteration #37000\n",
      "Iteration #38000\n",
      "Iteration #39000\n",
      "Iteration #40000\n",
      "Iteration #41000\n",
      "Iteration #42000\n",
      "Iteration #43000\n",
      "Iteration #44000\n",
      "Iteration #45000\n",
      "Iteration #46000\n",
      "Iteration #47000\n",
      "Iteration #48000\n",
      "Iteration #49000\n",
      "Iteration #50000\n",
      "Iteration #51000\n",
      "Iteration #52000\n",
      "Iteration #53000\n",
      "Iteration #54000\n",
      "Iteration #55000\n",
      "Iteration #56000\n",
      "Iteration #57000\n",
      "Iteration #58000\n",
      "Iteration #59000\n",
      "Iteration #60000\n",
      "Iteration #61000\n",
      "Iteration #62000\n",
      "Iteration #63000\n",
      "Iteration #64000\n",
      "Iteration #65000\n",
      "Iteration #66000\n",
      "Iteration #67000\n",
      "Iteration #68000\n",
      "Iteration #69000\n",
      "Iteration #70000\n",
      "Iteration #71000\n",
      "Iteration #72000\n",
      "Iteration #73000\n",
      "Iteration #74000\n",
      "Iteration #75000\n",
      "Iteration #76000\n",
      "Iteration #77000\n",
      "Iteration #78000\n",
      "Iteration #79000\n",
      "Iteration #80000\n",
      "Iteration #81000\n",
      "Iteration #82000\n",
      "Iteration #83000\n",
      "Iteration #84000\n",
      "Iteration #85000\n",
      "Iteration #86000\n",
      "Iteration #87000\n",
      "Iteration #88000\n",
      "Iteration #89000\n",
      "Iteration #90000\n",
      "Iteration #91000\n",
      "Iteration #92000\n",
      "Iteration #93000\n",
      "Iteration #94000\n",
      "Iteration #95000\n",
      "Iteration #96000\n",
      "Iteration #97000\n",
      "Iteration #98000\n",
      "Iteration #99000\n",
      "Iteration #100000\n",
      "Iteration #101000\n",
      "Iteration #102000\n",
      "Iteration #103000\n",
      "Iteration #104000\n",
      "Iteration #105000\n",
      "Iteration #106000\n",
      "Iteration #107000\n",
      "Iteration #108000\n",
      "Iteration #109000\n",
      "Iteration #110000\n",
      "Iteration #111000\n",
      "Iteration #112000\n",
      "Iteration #113000\n",
      "Iteration #114000\n",
      "Iteration #115000\n",
      "Iteration #116000\n",
      "Iteration #117000\n",
      "Iteration #118000\n",
      "Iteration #119000\n",
      "Iteration #120000\n",
      "Iteration #121000\n",
      "Iteration #122000\n",
      "Iteration #123000\n",
      "Iteration #124000\n",
      "Iteration #125000\n",
      "Iteration #126000\n",
      "Iteration #127000\n",
      "Iteration #128000\n",
      "Iteration #129000\n",
      "Iteration #130000\n",
      "Iteration #131000\n",
      "Iteration #132000\n",
      "Iteration #133000\n",
      "Iteration #134000\n",
      "Iteration #135000\n",
      "Iteration #136000\n",
      "Iteration #137000\n",
      "Iteration #138000\n",
      "Iteration #139000\n",
      "Iteration #140000\n",
      "Iteration #141000\n",
      "Iteration #142000\n",
      "Iteration #143000\n",
      "Iteration #144000\n",
      "Iteration #145000\n",
      "Iteration #146000\n",
      "Iteration #147000\n",
      "Iteration #148000\n",
      "Iteration #149000\n",
      "Iteration #150000\n",
      "Iteration #151000\n",
      "Iteration #152000\n",
      "Iteration #153000\n",
      "Iteration #154000\n",
      "Iteration #155000\n",
      "Iteration #156000\n",
      "Iteration #157000\n",
      "Iteration #158000\n",
      "Iteration #159000\n",
      "Iteration #160000\n",
      "Iteration #161000\n",
      "Iteration #162000\n",
      "Iteration #163000\n",
      "Iteration #164000\n",
      "Iteration #165000\n",
      "Iteration #166000\n",
      "Iteration #167000\n",
      "Iteration #168000\n",
      "Iteration #169000\n",
      "Iteration #170000\n",
      "Iteration #171000\n",
      "Iteration #172000\n",
      "Iteration #173000\n",
      "Iteration #174000\n",
      "Iteration #175000\n",
      "Iteration #176000\n",
      "Iteration #177000\n",
      "Iteration #178000\n",
      "Iteration #179000\n",
      "Iteration #180000\n",
      "Iteration #181000\n",
      "Iteration #182000\n",
      "Iteration #183000\n",
      "Iteration #184000\n",
      "Iteration #185000\n",
      "Iteration #186000\n",
      "Iteration #187000\n",
      "Iteration #188000\n",
      "Iteration #189000\n",
      "Iteration #190000\n",
      "Iteration #191000\n",
      "Iteration #192000\n",
      "Iteration #193000\n",
      "Iteration #194000\n",
      "Iteration #195000\n",
      "Iteration #196000\n",
      "Iteration #197000\n",
      "Iteration #198000\n",
      "Iteration #199000\n",
      "Iteration #200000\n",
      "Iteration #201000\n",
      "Iteration #202000\n",
      "Iteration #203000\n",
      "Iteration #204000\n",
      "Iteration #205000\n",
      "Iteration #206000\n",
      "Iteration #207000\n",
      "Iteration #208000\n",
      "Iteration #209000\n",
      "Iteration #210000\n",
      "Iteration #211000\n",
      "Iteration #212000\n",
      "Iteration #213000\n",
      "Iteration #214000\n",
      "Iteration #215000\n",
      "Iteration #216000\n",
      "Iteration #217000\n",
      "Iteration #218000\n",
      "Iteration #219000\n",
      "Iteration #220000\n",
      "Iteration #221000\n",
      "Iteration #222000\n",
      "Iteration #223000\n",
      "Iteration #224000\n",
      "Iteration #225000\n",
      "Iteration #226000\n",
      "Iteration #227000\n",
      "Iteration #228000\n",
      "Iteration #229000\n",
      "Iteration #230000\n",
      "Iteration #231000\n",
      "Iteration #232000\n",
      "Iteration #233000\n",
      "Iteration #234000\n",
      "Iteration #235000\n",
      "Iteration #236000\n",
      "Iteration #237000\n",
      "Iteration #238000\n",
      "Iteration #239000\n",
      "Iteration #240000\n",
      "Iteration #241000\n",
      "Iteration #242000\n",
      "Iteration #243000\n",
      "Iteration #244000\n",
      "Iteration #245000\n",
      "Iteration #246000\n",
      "Iteration #247000\n",
      "Iteration #248000\n",
      "Iteration #249000\n",
      "Iteration #250000\n",
      "Iteration #251000\n",
      "Iteration #252000\n",
      "Iteration #253000\n",
      "Iteration #254000\n",
      "Iteration #255000\n",
      "Iteration #256000\n",
      "Iteration #257000\n",
      "Iteration #258000\n",
      "Iteration #259000\n",
      "Iteration #260000\n",
      "Iteration #261000\n",
      "Iteration #262000\n",
      "Iteration #263000\n",
      "Iteration #264000\n",
      "Iteration #265000\n",
      "Iteration #266000\n",
      "Iteration #267000\n",
      "Iteration #268000\n",
      "Iteration #269000\n",
      "Iteration #270000\n",
      "Iteration #271000\n",
      "Iteration #272000\n",
      "Iteration #273000\n",
      "Iteration #274000\n",
      "Iteration #275000\n",
      "Iteration #276000\n",
      "Iteration #277000\n",
      "Iteration #278000\n",
      "Iteration #279000\n",
      "Iteration #280000\n",
      "Iteration #281000\n",
      "Iteration #282000\n",
      "Iteration #283000\n",
      "Iteration #284000\n",
      "Iteration #285000\n",
      "Iteration #286000\n",
      "Iteration #287000\n",
      "Iteration #288000\n",
      "Iteration #289000\n",
      "Iteration #290000\n",
      "Iteration #291000\n",
      "Iteration #292000\n",
      "Iteration #293000\n",
      "Iteration #294000\n",
      "Iteration #295000\n",
      "Iteration #296000\n",
      "Iteration #297000\n",
      "Iteration #298000\n",
      "Iteration #299000\n",
      "Iteration #300000\n"
     ]
    }
   ],
   "source": [
    "est1, alphas = sgd(obj, mixture, grad, zero_p, proj, max_iter= 3*10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ee0a3a0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(est1[:])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010162005629176738"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(alphas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([188, 127, 154, 146, 183, 191,  84, 129,  93,  88], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(probas)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second optimization task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Owen's realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simul_num1 = 10**4\n",
    "simul_num2 = 10**5\n",
    "dim_alpha = NUM_B + 1\n",
    "dim_beta = NUM_B\n",
    "dim_gamma = NUM_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = 1e-2\n",
    "delta = (1 + eta - (NUM_B + 1)*eps) / (NUM_B + 2)\n",
    "zero_alpha = (eps + delta) * np.ones((NUM_B + 1)) # zero alpha for optimization\n",
    "sample_alpha = np.ones((NUM_B + 1)) / (NUM_B + 1) #alpha which will be used for the first simulation stage\n",
    "sample_alpha[np.argmax(sample_alpha)] += 1 - sample_alpha.sum()\n",
    "mixture.set_alpha(sample_alpha)\n",
    "coefs = calculate_coefs(mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update1(x, direction, t):\n",
    "    sizes = tuple(len(x_i) for x_i in x)\n",
    "    start_idx = []\n",
    "    idx = 0\n",
    "    for i in range(len(x)):\n",
    "        start_idx.append(idx)\n",
    "        idx += sizes[i]\n",
    "    return [x[i] + t*direction[start_idx[i]: start_idx[i] + sizes[i]] for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho: 1\n",
      "[[ 1293.37851547]]\n",
      "[[ 0.1369555]]\n",
      "rho: 0.5\n",
      "rho: 0.25\n",
      "rho: 0.125\n",
      "[[ 0.00011247]]\n",
      "rho: 0.0625\n",
      "rho: 0.03125\n",
      "rho: 0.015625\n",
      "[[ 0.0002085]]\n",
      "rho: 0.0078125\n",
      "rho: 0.00390625\n",
      "[[ 0.00018089]]\n",
      "rho: 0.001953125\n",
      "rho: 0.0009765625\n",
      "[[ 0.00013474]]\n",
      "rho: 0.00048828125\n",
      "rho: 0.000244140625\n",
      "rho: 0.0001220703125\n",
      "[[ 0.00010632]]\n",
      "rho: 6.103515625e-05\n",
      "rho: 3.0517578125e-05\n",
      "rho: 1.52587890625e-05\n",
      "rho: 7.62939453125e-06\n",
      "rho: 3.814697265625e-06\n",
      "rho: 1.9073486328125e-06\n",
      "rho: 9.5367431640625e-07\n",
      "[[ 0.0001022]]\n",
      "rho: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "#eta = 1e-3\n",
    "tol_e = 1e-4\n",
    "rho = 1\n",
    "zero_beta = np.zeros((NUM_B))\n",
    "zero_gamma = np.zeros((NUM_B))\n",
    "dom_func = lambda x: np.all(x[0] >= eps) & (x[0].sum() < 1 + eta)\n",
    "params_to_opt = (zero_alpha, zero_beta, zero_gamma)\n",
    "while True:\n",
    "    print('rho:', rho)\n",
    "    func_wrap = lambda x: big_func(coefs, x[0], x[1], x[2], rho = rho)\n",
    "    grad_wrap = lambda x: big_grad(coefs, x[0], x[1], x[2], rho = rho)\n",
    "    hess_wrap = lambda x: big_hess(coefs, x[0], x[1], x[2], rho = rho)\n",
    "    params_to_opt = damped_newton(func_wrap, grad_wrap, hess_wrap, dom_func, params_to_opt, update1)\n",
    "    if (NUM_B + 2) * rho < func(coefs, params_to_opt[0], params_to_opt[1], params_to_opt[2]) * tol_e:\n",
    "        break\n",
    "    else: rho = rho / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010350642880723925"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = SimplexProjector(eps)\n",
    "get_estimation(mixture, proj, params_to_opt[0], params_to_opt[1], params_to_opt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound of estimation 0.010396508968529443 \n",
      " lower bound of estimation 0.0012844157635156065\n"
     ]
    }
   ],
   "source": [
    "print('upper bound of estimation {0} \\n lower bound of estimation {1}'.format(probas.sum(), probas.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sgd for second optimization task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = lambda x, params: obj1(x, mixture, params[0], params[1], params[2])\n",
    "grad = lambda x, params: grad1(x, mixture, params[0], params[1], params[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_a = (1 / (NUM_B + 1)) * np.ones((NUM_B + 1))\n",
    "zero_b = np.zeros((NUM_B))\n",
    "zero_g = np.zeros((NUM_B))\n",
    "zero_p = (zero_a, zero_b, zero_g)\n",
    "proj = SimplexProjector(epsilon = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1000\n",
      "Iteration #2000\n",
      "Iteration #3000\n",
      "Iteration #4000\n",
      "Iteration #5000\n",
      "Iteration #6000\n",
      "Iteration #7000\n",
      "Iteration #8000\n",
      "Iteration #9000\n",
      "Iteration #10000\n",
      "Iteration #11000\n",
      "Iteration #12000\n",
      "Iteration #13000\n",
      "Iteration #14000\n",
      "Iteration #15000\n",
      "Iteration #16000\n",
      "Iteration #17000\n",
      "Iteration #18000\n",
      "Iteration #19000\n",
      "Iteration #20000\n",
      "Iteration #21000\n",
      "Iteration #22000\n",
      "Iteration #23000\n",
      "Iteration #24000\n",
      "Iteration #25000\n",
      "Iteration #26000\n",
      "Iteration #27000\n",
      "Iteration #28000\n",
      "Iteration #29000\n",
      "Iteration #30000\n",
      "Iteration #31000\n",
      "Iteration #32000\n",
      "Iteration #33000\n",
      "Iteration #34000\n",
      "Iteration #35000\n",
      "Iteration #36000\n",
      "Iteration #37000\n",
      "Iteration #38000\n",
      "Iteration #39000\n",
      "Iteration #40000\n",
      "Iteration #41000\n",
      "Iteration #42000\n",
      "Iteration #43000\n",
      "Iteration #44000\n",
      "Iteration #45000\n",
      "Iteration #46000\n",
      "Iteration #47000\n",
      "Iteration #48000\n",
      "Iteration #49000\n",
      "Iteration #50000\n",
      "Iteration #51000\n",
      "Iteration #52000\n",
      "Iteration #53000\n",
      "Iteration #54000\n",
      "Iteration #55000\n",
      "Iteration #56000\n",
      "Iteration #57000\n",
      "Iteration #58000\n",
      "Iteration #59000\n",
      "Iteration #60000\n",
      "Iteration #61000\n",
      "Iteration #62000\n",
      "Iteration #63000\n",
      "Iteration #64000\n",
      "Iteration #65000\n",
      "Iteration #66000\n",
      "Iteration #67000\n",
      "Iteration #68000\n",
      "Iteration #69000\n",
      "Iteration #70000\n",
      "Iteration #71000\n",
      "Iteration #72000\n",
      "Iteration #73000\n",
      "Iteration #74000\n",
      "Iteration #75000\n",
      "Iteration #76000\n",
      "Iteration #77000\n",
      "Iteration #78000\n",
      "Iteration #79000\n",
      "Iteration #80000\n",
      "Iteration #81000\n",
      "Iteration #82000\n",
      "Iteration #83000\n",
      "Iteration #84000\n",
      "Iteration #85000\n",
      "Iteration #86000\n",
      "Iteration #87000\n",
      "Iteration #88000\n",
      "Iteration #89000\n",
      "Iteration #90000\n",
      "Iteration #91000\n",
      "Iteration #92000\n",
      "Iteration #93000\n",
      "Iteration #94000\n",
      "Iteration #95000\n",
      "Iteration #96000\n",
      "Iteration #97000\n",
      "Iteration #98000\n",
      "Iteration #99000\n",
      "Iteration #100000\n"
     ]
    }
   ],
   "source": [
    "est2, params2 = sgd(obj, mixture, grad, zero_p, proj, max_iter= 10**5, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25ee0a178d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(est2[-10000:])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16202.889779438994"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04,   9.00510204e-01,   5.10204082e-04,\n",
       "          5.10204082e-04,   5.10204082e-04,   5.10204082e-04,\n",
       "          5.10204082e-04]),\n",
       " array([ -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48355432e-04,  -4.57618027e-03,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48639939e-04,  -2.48356647e-04,\n",
       "         -2.48357421e-04,  -2.48356647e-04,  -2.48356251e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48355911e-04,\n",
       "         -2.48356647e-04,  -2.48740210e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48482452e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48354856e-04,   1.26404580e+02,  -2.48356647e-04,\n",
       "         -2.48356647e-04,   5.82414012e+01,  -2.48356647e-04,\n",
       "         -2.48357346e-04,  -2.48636665e-04,  -2.48356647e-04,\n",
       "         -2.48356663e-04,  -2.48356647e-04,  -1.18878710e-02,\n",
       "         -2.48357491e-04,   2.38023667e+02,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356649e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356665e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,   1.45912910e+02,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -1.71826905e-02,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356405e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -1.24955543e-02,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356183e-04,  -2.48356647e-04,  -2.48356738e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,   2.07171769e+02,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.93783498e-04,  -2.48357366e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,   9.90158119e+00,\n",
       "         -2.48357285e-04,  -2.48356741e-04,  -2.48356647e-04,\n",
       "          8.97555147e-03,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48380013e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356661e-04,  -1.00602489e-02,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,   2.21606493e+02,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48081398e-04,  -1.15201320e-02,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,   2.21632539e+02,\n",
       "          1.73365266e+02,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,   2.32313069e+02,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,   4.06711816e+01,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.62317636e-04,\n",
       "         -2.58869367e-03,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -2.48356647e-04,  -2.48356647e-04,\n",
       "         -2.48356647e-04,   3.80391606e+01,  -2.48356657e-04,\n",
       "         -2.48357451e-04,   1.35429107e+02,  -2.48356647e-04,\n",
       "         -2.48356647e-04,  -5.92581788e+01,  -2.48356647e-04,\n",
       "          2.42750035e+02,  -2.48357235e-04,  -2.48355562e-04]),\n",
       " array([-0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903,\n",
       "        -0.00334903, -0.00334903, -0.00334903, -0.00334903, -0.00334903])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = calculate_coefs(mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9283217160061.373"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(coefs, params2[-1][0], params2[-1][1], params2[-1][2], sample_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.813629599442049"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(coefs, params2[0][0], sample_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-72.196769596372405"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = SimplexProjector(eps)\n",
    "get_estimation(mixture, proj, params2[-1][0], params2[-1][1], params2[-1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01036219829682803"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_alore = np.zeros((NUM_B + 1))\n",
    "alpha_alore[:-1] = probas / probas.sum()\n",
    "get_estimation(mixture, proj, alpha_alore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound of estimation 0.010396508968529443 \n",
      " lower bound of estimation 0.0012844157635156065\n"
     ]
    }
   ],
   "source": [
    "print('upper bound of estimation {0} \\n lower bound of estimation {1}'.format(probas.sum(), probas.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
